{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook documents the processing of the raw results from the activation scheme/parcel model sampling data into tidy datasets more amenable for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original sampling datasets and their immediate post-processing forms have been copied from `legion` and are archived in **data/{exp_name}**. Additionally, the aerosol sampling datasets have been copied to **data/aerosol_sampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling data is organized into two different sets:\n",
    "\n",
    "1. **aerosol_design** - Sampling experiment conducted by sub-sampling aerosol/meteorology conditions from MARC output\n",
    "2. **LHS_design** - Sampling experiment where LHS has been applied to the chaos expansion input parameter spaces.\n",
    "\n",
    "For the **LHS_design** sampling results, we care about just one file for each scheme,  `data/{scheme_name}/{scheme_name}_sampling_results.csv`, which contains the design as well as all the sampling results. For the **aerosol_design** dataset, we care about four different datasets: \n",
    "\n",
    "1. `MARC_aerosol_design.csv` - sampling input parameters, for *both* \"main\" and \"gCCN\" modes.\n",
    "2. `MARC_main_aerosol_design_parameterization_results.csv` - Evaluation of **MARC_main** against aerosol samples, and parameterizatons evaluted using *only* \"main\" modes.\n",
    "3. `MARC_gCCN_aerosol_design_parameterization_results.csv` - Same as above, but for **MARC_gCCN**\n",
    "4. `MARC_gCCN_aerosol_design_parcel_results.csv` - Parcel model evaluations of *full* \"gCCN\" distribution samples.\n",
    "\n",
    "--- \n",
    "\n",
    "Our objective here is to re-format these datasets for easier visualization and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-processing\n",
    "\n",
    "Each of the two data sets is processed separately.\n",
    "\n",
    "---\n",
    "\n",
    "## LHS_design\n",
    "\n",
    "The **LHS_design** results are the evaluation datasets for the two activation schemes. We want to pivot the dataset into a tidy format, where each row is a \"sample\" result. Each row should encode the following information:\n",
    "\n",
    "- Scheme name generating sample (main\\_*n*, ARG, parcel, etc)\n",
    "- Variable being analyzed\n",
    "    - Smax, Neq, Nkn - directly predicted by scheme\n",
    "    - Nderiv - Droplet number diagnosed from Smax prediction\n",
    "- Sample id, based on associated **design** table\n",
    "- Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually set exp_name\n",
      "Index(['logN_ACC', 'logN_MOS', 'logN_MBS', 'logN_DST01', 'logN_DST02',\n",
      "       'logN_SSLT01', 'logmu_ACC', 'logmu_MOS', 'logmu_MBS', 'kappa_MOS',\n",
      "       'logV', 'T', 'P', 'Smax_parcel', 'Neq_parcel', 'Nkn_parcel',\n",
      "       'Smax_gCCN_3', 'Nderiv_gCCN_3', 'Neq_gCCN_3', 'Nkn_gCCN_3',\n",
      "       'Smax_gCCN_4', 'Nderiv_gCCN_4', 'Neq_gCCN_4', 'Nkn_gCCN_4', 'Smax_ARG',\n",
      "       'Nderiv_ARG', 'Smax_MBN', 'Nderiv_MBN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "if \"exp_name\" not in environ:\n",
    "    exp_name = \"gCCN\"\n",
    "    print(\"Manually set exp_name\")\n",
    "else:\n",
    "    exp_name = environ['exp_name']\n",
    "orders = [3, 4]\n",
    "\n",
    "def get_fn(basename, scheme):\n",
    "    full_path = os.path.join(\"data/MARC_{scheme}/\", basename)\n",
    "    return full_path.format(scheme=scheme)\n",
    "\n",
    "sampling_fn = get_fn(\"MARC_{scheme}_sampling_results.csv\", exp_name)\n",
    "sampling_df = pd.read_csv(sampling_fn, index_col=0)\n",
    "\n",
    "exp_dict_fn = get_fn(\"MARC_{scheme}.p\", exp_name)\n",
    "with open(exp_dict_fn, 'rb') as f:\n",
    "    exp_dict = pickle.load(f)\n",
    "    \n",
    "print(sampling_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the \"design\" (variables) from the sampling data into their own dataframe. Their index will become the \"sample id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logN_ACC</th>\n",
       "      <th>logN_MOS</th>\n",
       "      <th>logN_MBS</th>\n",
       "      <th>logN_DST01</th>\n",
       "      <th>logN_DST02</th>\n",
       "      <th>logN_SSLT01</th>\n",
       "      <th>logmu_ACC</th>\n",
       "      <th>logmu_MOS</th>\n",
       "      <th>logmu_MBS</th>\n",
       "      <th>kappa_MOS</th>\n",
       "      <th>logV</th>\n",
       "      <th>T</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.882104</td>\n",
       "      <td>-4.787196</td>\n",
       "      <td>1.279272</td>\n",
       "      <td>0.499861</td>\n",
       "      <td>0.894579</td>\n",
       "      <td>-1.424097</td>\n",
       "      <td>-1.080309</td>\n",
       "      <td>-1.340698</td>\n",
       "      <td>-2.636864</td>\n",
       "      <td>0.583582</td>\n",
       "      <td>0.128252</td>\n",
       "      <td>241.004204</td>\n",
       "      <td>90352.20554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.228576</td>\n",
       "      <td>3.424074</td>\n",
       "      <td>2.819142</td>\n",
       "      <td>0.176743</td>\n",
       "      <td>0.147766</td>\n",
       "      <td>-1.513440</td>\n",
       "      <td>-0.914070</td>\n",
       "      <td>-1.905798</td>\n",
       "      <td>-2.882728</td>\n",
       "      <td>0.317569</td>\n",
       "      <td>0.725811</td>\n",
       "      <td>308.554350</td>\n",
       "      <td>102805.05600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.663175</td>\n",
       "      <td>-0.909762</td>\n",
       "      <td>0.991011</td>\n",
       "      <td>-4.091173</td>\n",
       "      <td>-3.424029</td>\n",
       "      <td>-0.858034</td>\n",
       "      <td>-0.193682</td>\n",
       "      <td>-1.668761</td>\n",
       "      <td>-2.803899</td>\n",
       "      <td>0.543952</td>\n",
       "      <td>0.796432</td>\n",
       "      <td>245.981379</td>\n",
       "      <td>77480.99490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.719233</td>\n",
       "      <td>3.404198</td>\n",
       "      <td>-3.240300</td>\n",
       "      <td>-0.878332</td>\n",
       "      <td>0.234148</td>\n",
       "      <td>-2.540045</td>\n",
       "      <td>-1.879270</td>\n",
       "      <td>-1.291892</td>\n",
       "      <td>-1.580388</td>\n",
       "      <td>0.148080</td>\n",
       "      <td>0.374067</td>\n",
       "      <td>284.446532</td>\n",
       "      <td>84903.94172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.495877</td>\n",
       "      <td>1.058844</td>\n",
       "      <td>0.179819</td>\n",
       "      <td>0.280961</td>\n",
       "      <td>-1.618518</td>\n",
       "      <td>-2.498890</td>\n",
       "      <td>-2.070286</td>\n",
       "      <td>-2.803842</td>\n",
       "      <td>-2.493860</td>\n",
       "      <td>0.372425</td>\n",
       "      <td>-1.763832</td>\n",
       "      <td>292.536824</td>\n",
       "      <td>50851.32708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           logN_ACC  logN_MOS  logN_MBS  logN_DST01  logN_DST02  logN_SSLT01  \\\n",
       "sample_id                                                                      \n",
       "0          1.882104 -4.787196  1.279272    0.499861    0.894579    -1.424097   \n",
       "1         -1.228576  3.424074  2.819142    0.176743    0.147766    -1.513440   \n",
       "2          2.663175 -0.909762  0.991011   -4.091173   -3.424029    -0.858034   \n",
       "3          3.719233  3.404198 -3.240300   -0.878332    0.234148    -2.540045   \n",
       "4          3.495877  1.058844  0.179819    0.280961   -1.618518    -2.498890   \n",
       "\n",
       "           logmu_ACC  logmu_MOS  logmu_MBS  kappa_MOS      logV           T  \\\n",
       "sample_id                                                                     \n",
       "0          -1.080309  -1.340698  -2.636864   0.583582  0.128252  241.004204   \n",
       "1          -0.914070  -1.905798  -2.882728   0.317569  0.725811  308.554350   \n",
       "2          -0.193682  -1.668761  -2.803899   0.543952  0.796432  245.981379   \n",
       "3          -1.879270  -1.291892  -1.580388   0.148080  0.374067  284.446532   \n",
       "4          -2.070286  -2.803842  -2.493860   0.372425 -1.763832  292.536824   \n",
       "\n",
       "                      P  \n",
       "sample_id                \n",
       "0           90352.20554  \n",
       "1          102805.05600  \n",
       "2           77480.99490  \n",
       "3           84903.94172  \n",
       "4           50851.32708  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = [v[0] for v in exp_dict['variables']]\n",
    "\n",
    "design_df = sampling_df[vs]\n",
    "design_df = (\n",
    "    design_df\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"sample_id\"})\n",
    "    .set_index(\"sample_id\")\n",
    ")\n",
    "design_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now manually construct the tidy samplying dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = design_df.index\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Parcel model\n",
    "parcel_data = (\n",
    "    # Select just the fields for this scheme\n",
    "    sampling_df[['Smax_parcel', 'Neq_parcel']]\n",
    "    # Re-name fields prior to reshaping\n",
    "    .rename(columns={'Smax_parcel': 'Smax',\n",
    "                     'Neq_parcel': 'Nderiv'})\n",
    "    # Set the index to the \"sample_id\" index from the design,\n",
    "    # and move to named column\n",
    "    .set_index(idx).reset_index()\n",
    "    # Add a column identifying this scheme\n",
    "    .assign(scheme=\"parcel\")\n",
    "    # Re-shape into tidy form\n",
    "    .pipe(pd.melt, id_vars=['scheme', 'sample_id'])\n",
    ")\n",
    "all_data.append(parcel_data)\n",
    "\n",
    "# Parameterizations\n",
    "scheme_data = []\n",
    "for scheme in [\"ARG\", \"MBN\"]:\n",
    "    df = (\n",
    "        sampling_df[['Smax_'+scheme, 'Nderiv_'+scheme]]\n",
    "        .rename(columns={'Smax_'+scheme: 'Smax',\n",
    "                         'Nderiv_'+scheme: 'Nderiv'})\n",
    "        .set_index(idx).reset_index()\n",
    "        .assign(scheme=scheme)\n",
    "        .pipe(pd.melt, id_vars=['scheme', 'sample_id'])\n",
    "    )\n",
    "    scheme_data.append(df)\n",
    "all_data.extend(scheme_data)\n",
    "\n",
    "# Chaos Expansions\n",
    "chaos_data = []\n",
    "for scheme in [\"{}_{}\".format(exp_name, order) for order in orders]:\n",
    "    df = (\n",
    "        sampling_df[['Smax_'+scheme, 'Nderiv_'+scheme]]\n",
    "        .rename(columns={'Smax_'+scheme: 'Smax',\n",
    "                         'Nderiv_'+scheme: 'Nderiv'})\n",
    "        .set_index(idx).reset_index()\n",
    "        .assign(scheme=scheme)\n",
    "        .pipe(pd.melt, id_vars=['scheme', 'sample_id'])\n",
    "    )\n",
    "    chaos_data.append(df)\n",
    "all_data.extend(chaos_data)\n",
    "\n",
    "# Concatenate into single DataFrame\n",
    "all_data = pd.concat(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: sampling_data.h5\n",
      "/gCCN/design             frame        (shape->[10000,13])\n",
      "/gCCN/results            frame        (shape->[100000,4])\n",
      "/main/design             frame        (shape->[10000,10])\n",
      "/main/results            frame        (shape->[100000,4])\n"
     ]
    }
   ],
   "source": [
    "hdf = pd.HDFStore('data/MARC_{}_sampling_data.h5'.format(exp_name))\n",
    "hdf.put(\"/design\", design_df)\n",
    "hdf.put(\"/results\", all_data)\n",
    "\n",
    "print(hdf)\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Sample code for compute statistics over the sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "    subset = all_data.set_index([\"variable\", \"scheme\", \"sample_id\"]).loc['Smax']Ë˜\n",
    "\n",
    "    ref = subset.loc['parcel']\n",
    "    groups = subset.groupby(level=\"scheme\")\n",
    "\n",
    "    import sklearn.metrics as skm\n",
    "    import numpy as np\n",
    "\n",
    "    def compute_stats(obs, act):\n",
    "        \"\"\" Create a dictionary with summary statistics comparing two\n",
    "        datasets. \"\"\"\n",
    "\n",
    "        obs = np.asarray(obs)\n",
    "        act = np.asarray(act)\n",
    "\n",
    "        mae = skm.mean_absolute_error(act, obs)\n",
    "        r2 = skm.r2_score(act, obs)\n",
    "        rmse = np.sqrt(np.sum((obs-act)**2.)/len(act))\n",
    "        nrmse = rmse/np.sqrt(np.sum((act**2.)/len(act)))\n",
    "\n",
    "        rel_err = 100.*(obs - act)/act\n",
    "        # Mask egregiously high values (1000% error) which screw up the spread\n",
    "        rel_err = rel_err[np.abs(rel_err) <= 1000.]\n",
    "        mre = np.mean(rel_err)\n",
    "        mre_std = np.std(rel_err)\n",
    "\n",
    "        stats = pd.Series({\n",
    "            'mae': mae, 'r2': r2, 'rmse': rmse, 'nrmse': nrmse,\n",
    "            'mre': mre, 'mre_std': mre_std,\n",
    "        })\n",
    "\n",
    "        return stats\n",
    "\n",
    "\n",
    "    x = groups.apply(lambda x: compute_stats(ref, x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## aerosol_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **aerosol_sampling** results come from evaluating the activation schemes on sample aerosol distributions from MARC. Again, we want to pivot the dataset into a tidy format, where each row is a \"sample\" result, such that each row should encode the following information:\n",
    "\n",
    "- Scheme name generating sample (main\\_*n*, ARG, parcel, etc)\n",
    "- Variable being analyzed\n",
    "    - Smax, Neq, Nkn - directly predicted by scheme\n",
    "    - Nderiv - Droplet number diagnosed from Smax prediction\n",
    "- Sample id, based on associated **design** table\n",
    "- Value\n",
    "\n",
    "First we need to load in the tabular data, which is contained in separate\n",
    "files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_data = pd.read_csv(\"data/aerosol_sampling/MARC_main_aerosol_design_parameterization_results.csv\",\n",
    "                        index_col=0)\n",
    "gCCN_data = pd.read_csv(\"data/aerosol_sampling/MARC_gCCN_aerosol_design_parameterization_results.csv\",\n",
    "                        index_col=0)\n",
    "parcel_data = pd.read_csv(\"data/aerosol_sampling/MARC_gCCN_aerosol_design_parcel_results.csv\",\n",
    "                          index_col=0)\n",
    "design_df = (\n",
    "    pd.read_csv(\"data/aerosol_sampling/MARC_aerosol_design.csv\")\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'sample_id'})\n",
    "    .set_index('sample_id')\n",
    ")\n",
    "\n",
    "idx = design_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process these just like with the previous sampling experiment. The parcel model simulations are straightforward; we'll use the parameterizations from the *gCCN* sampling case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "all_data = []\n",
    "\n",
    "# Parcel data\n",
    "scheme = 'parcel'\n",
    "parcel_df = (\n",
    "    parcel_data[['Smax_'+scheme, 'Neq_'+scheme]]\n",
    "    .rename(columns={'Smax_'+scheme: 'Smax',\n",
    "                     'Neq_'+scheme: 'Nderiv'})\n",
    "    .set_index(idx).reset_index()\n",
    "    .assign(scheme=scheme)\n",
    "    .pipe(pd.melt, id_vars=['scheme', 'sample_id'])\n",
    ")\n",
    "all_data.append(parcel_df)\n",
    "\n",
    "# Parameterization data\n",
    "scheme_data = []\n",
    "for scheme in [\"ARG\", \"MBN\"]:\n",
    "    df = (\n",
    "        gCCN_data[['Smax_'+scheme, 'Nderiv_'+scheme]]\n",
    "        .rename(columns={'Smax_'+scheme: 'Smax',\n",
    "                         'Nderiv_'+scheme: 'Nderiv'})\n",
    "        .set_index(idx).reset_index()\n",
    "        .assign(scheme=scheme)\n",
    "        .pipe(pd.melt, id_vars=['scheme', 'sample_id'])\n",
    "    )\n",
    "    scheme_data.append(df)\n",
    "all_data.extend(scheme_data)\n",
    "\n",
    "# Chaos Expansions\n",
    "chaos_data = []\n",
    "for scheme, order in product(['main', 'gCCN'],\n",
    "                             [3, 4]):\n",
    "    df = gCCN_data if scheme == \"gCCN\" else main_data\n",
    "    data = (\n",
    "        df[['Smax_expansion_order_{}'.format(order), \n",
    "            'Nderiv_expansion_order_{}'.format(order)]]\n",
    "        .rename(columns={\n",
    "                'Smax_expansion_order_{}'.format(order): 'Smax',\n",
    "                'Nderiv_expansion_order_{}'.format(order): 'Nderiv'\n",
    "        })\n",
    "        .set_index(idx).reset_index()\n",
    "        .assign(scheme=\"{}_{}\".format(scheme, order))\n",
    "        .pipe(pd.melt, id_vars=['scheme', 'sample_id'])\n",
    "    )\n",
    "        \n",
    "    # There's a small glitch in the output; for the 4th order\n",
    "    # schemes, a vector of \n",
    "    #    [Smax_arg, Nact_arg, Smax_mbn, Nact_mbn, Nderiv]\n",
    "    # is reported instead of the Nderiv value, so we need\n",
    "    # to select just that item\n",
    "    if (order == 4):\n",
    "        def _sel_last(x):\n",
    "            x = [float(elem) for elem in x[1:-1].split(\",\")]\n",
    "            return x[-1]\n",
    "        x = data.loc[data.variable == 'Nderiv'].value\n",
    "        print(order)\n",
    "        data.loc[data.variable == 'Nderiv', 'value'] = x.apply(_sel_last)\n",
    "    chaos_data.append(data)\n",
    "all_data.extend(chaos_data)\n",
    "\n",
    "# Concatenate into single DataFrame\n",
    "all_data = pd.concat(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to an HDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: data/aerosol_data.h5\n",
      "/design             frame        (shape->[10000,20])\n",
      "/results            frame        (shape->[140000,4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/anaconda/lib/python3.4/site-packages/IPython/core/interactiveshell.py:2809: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->['scheme', 'variable', 'value']]\n",
      "\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "hdf = pd.HDFStore('data/aerosol_data.h5', mode='w')\n",
    "hdf.put(\"/design\", design_df)\n",
    "hdf.put(\"/results\", all_data)\n",
    "\n",
    "print(hdf)\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Sample code for creating a one-one plot.\n",
    "\n",
    "``` python\n",
    "    import numpy as np\n",
    "\n",
    "    x = hdf['results'].set_index(['variable', 'scheme', 'sample_id'])\n",
    "    y = x.loc['Smax']\n",
    "    ref = y.loc['parcel'].value\n",
    "    test = y.loc['MBN'].value\n",
    "\n",
    "    df = pd.DataFrame({'ref': ref, 'test': test})\n",
    "    df[df > 10.] = np.nan\n",
    "    df[df < -10.] = np.nan\n",
    "    df = (\n",
    "        df\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "    ) \n",
    "\n",
    "    sns.jointplot('ref', 'test', df, kind='reg')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
